Week 1, Mon
========
첫수업, 교수님께서 대형강의를 좋아해서 온라인강의에 불만이 많다고 하신다.

알고리즘의 사전적 정의: 입력으로부터 출력을 만드는 과정을 애매하지 않게 기술한 것. 문제해결 = Data structure + Algorithm. 이 강의에선 알고리즘 자체를 공부하기도 하지만, 그만큼 중요하게 생각하는 방법을 익히게 될 것이다. 알고리즘을 공부하는건 현재의 문제를 해결하는법을 배우는것이지만, 생각하는 방법을 배우는 것은 미래의 문제를 미리 해결해두는것이나 마찬가지입니다.

이번 강의에서 다룰 중요한 주제들

1.  알고리즘 설계와 분석의 기초. 알고리즘에 대한 개략적인 이야기를 할것이다. 알고리즘 복잡도에 대해 다루게된다.
2.  Recurrences, 점화식. 재귀와 밀접한 관련이 있다. 재귀적 구조를 가진 알고리즘의 복잡도는 어떻게 계산하는가 이런것도 다룸.
3.  Sorting, 정렬. 자구에서도 했지만 더 깊게 배움. 알고리즘 각각의 설명, 복잡도 계산 및 증명, 각각의 아이디어를 깊게 다룸.
4.  Selection, 선택. "n개의 수 중에서 n/3번째로 작은 원소 찾기" 이런걸 다룸. 이런 알고리즘들을 소개함. O(n^2) 복잡도의 원시적인 알고리즘부터 O(nlogn), O(n) 등 다양한 알고리즘을 배움.
5.  Search Trees. 자구에서 배운것보다 더 깊게 다룸. 자료구조의 이론적 퍼포먼스 증명 등
6.  Hash Tables. 얘도 증명 포함됨
7.  Handling Exclusive Sets. 배타적인 집합을 다루는 방법.
8.  Dynamic Programming. 아무리 강조해도 지나치지 않음. "동적 프로그래밍"이라는 단어는 DP가 실제로 어떤것인지랑은 무관해서 부적합한 표현임. 재귀적인 구조를 가진 문제를 푸는 방법. 아주 중요함
9.  Graph Algorithms. DFS, BFS, 미니멈 스패닝 트리, 최단경로문제, Strongly connected component, 토폴로지 소팅 등 다양한 문제를 다루고, 알고리즘이 왜 맞는지 증명도 함.
10. Greedy Algorithms. 드물게 그리디한 접근을 해도 글로벌 옵티멈을 보장함. 프림 알고리즘, 크루스칼 알고리즘, 다익스트라 알고리즘. 이런 예가 많지 않은데 이런 그리디들을 배움. [매트로이드]라는 수학적인 구조를 가지면 어떤 성질로 도움을 받을 수 있는지, 이런걸 배움. 매트로이드는 그래프의 더 abstract한 mathematical object
11. String Matching. Exact matching을 배우면 Approximate matching 알고리즘도 만들 수 있음.
12. NP-Complete/Hard. 이 앞 챕터까지는 어떤 문제를 잘 풀수있을까에 대한 이야기인데, 이 챕터는 어떤 문제가 풀기 힘드냐를 배우게됨. "현실시간에 풀 수 없다"는 가정을 다들 믿고 살긴 하는데 그것도 증명이 안되어있어서 세계 7대 난제로 등록되어있음.
13. Problem Space Search. 문제를 푸는 Rugged한 공간에서 어떻게 이동할것인가, 어떻게 탐색을 할것인가에 대해 배움. 문제공간의 특성, 문제공간을 어떤 관점으로 보느냐.

[매트로이드]: https://en.wikipedia.org/wiki/Matroid

추천교재

1. 쉽게 배우는 알고리즘, 문병로, 한빛미디어
2. Introduction to Algorithms, Cormen et al, MIT Press.

숙제는 총 4회. 2회는 Handwriting, 2회는 프로그래밍.

보통 알고리즘 강의는 30명정도가 하는데, 지금 알고리즘 수업이 120명이 하는 대형강의가 되었다. 조교들이 본인 연구를 하면서 동시에 숙제 채점을 하기때문에 굉장히 일이 힘들다. 조교들이 대답이 늦더라도 양해를 부탁한다.

시험은 반드시 교실에서 봄. Open Book. 시험시간 들고올 수 있는것은 책 두 권, 강의 노트 프린트+메모, 기출 문제 및 자신이 풀어본 내용. 두 교실로 나눠서 시험을 보게됨. 디지털 미디어는 가져오면 안됨

&nbsp;

Week 1, Wed
========

&nbsp;

## 1. 알고리즘이란
알고리즘: 문제 해결 절차를 체계적으로 기술한것

### 알고리즘을 공부하는 목적

1.  특정 문제를 해결하는 방법을 배우기도 있지만
2.  체계적으로 생각하는 훈련을 하고
3.  지적 추상화 레벨의 상승(Intellectual abstraction)을 이룩할수있다. 연구나 개발에 있어 정신적 여유를 유지하기 위해 매우 중요한 요소다!

문교수님 왈 워낙 다양한 문제를 다루고 시행착오를 겪고 나니까, 새로운 문제 새로운 프로젝트를 다룰때에 "이걸 해결할 수 없으면 어쩌나"하는 느낌을 겪지 **못할까봐** 두렵다. 너무 쉬우면 제대로된 논문으로 나오지도 않고, 남에게 내세우지도 못하고, 자신에게 만족감도 줄수없고.

챌린징한 문제를 푸는 경험을 누적해나가는게 중요하다. 지금 님들한테 주는 과제가 처음 보면 꽤 어려울 수 있는데, 계속 머리에 넣어두고 계속 생각해보면 듀데이트가 될 즈음이 되면 어떻게든 풀게될겁니다. 머리속에 일단 넣어두고있으면 밥먹을때 버스탈때 잘때 잠재의식이 계속 풀고있을거다!

문제 자체를 해결하는 알고리즘을 배우는것도 중요하지만, 생각하는 방법을 배우는게 더 중요하다! 미래에 맞딱드릴 문제를 해결할 빌딩블록을 얻게될거다.

알고리즘은 자료구조의 확장이다. 자료구조는 건축이나 자동차 제작의 건축자재, 부품, 모듈같은것임. 이거를 조립해서 더 복잡하고 어려운 문제를 푸는거다.

&nbsp;

## 2. 알고리즘 설계와 분석의 기초

### 알고리즘 수행시간
바람직한 알고리즘은 명확해야하고, 가능하면 간명해야한다. 지나친 기호적인 표현들은 오히려 명확성을 떨어트린다. 명확하게 기술할 수 있다면 그냥 말로 풀어써도 됨.

바람직한 알고리즘은 효율적이어야한다. 수행시간 효율적이어야 하고 알고리즘이 차지하는 메모리 공간이 효율적이어야함. 이 알고리즘 클래스에선 수행시간에 주로 집중한다.

알고리즘의 수행시간에는 여러가지 클래스가 있다. log N, N, NlogN, N^2, N^3, 2^N, 등.

알고리즘 수행시간이 n^2/5, 5*n 이런식으로 주어진다면 상수때문에 입력이 작을때에는 n^2/5 알고리즘이 더 빠를수도있지만, 입력이 충분히 커지면 차수가 큰 알고리즘이 이기게된다. asymptotic(점근의) notation에서는 계수가 중요하지 않고 차수가 중요하다.

알고리즘의 수행시간은 for 문 반복회수, 특정 행이 실행되는 회수, 함수 호출회수 등 다양한 기준으로 결정할 수 있다.

(예제로 수행시간 계산하는 문제 여럿 풀어봄)

### 재귀와 귀납적 사고
Optimal Substructure: 큰 문제의 최적해가 자신보다 작은 문제의 최적해를 포함하는 구조

예제: 머지소트 `T(n) = 2*T(n/2) + cn`

```
T(n) = 2*T(n/2) + cn
     = 2^1 * T(n/2^1) + 1*cn
     = 2^2 * T(n/2^2) + 2*cn
     = 2^3 * T(n/2^3) + 3*cn
     ...
     = 2^k * T(n/2^k) + k*cn     (k = log_2 n)
     = 2^k * T(1)     + k*cn     (2^k = n)
     = a*n + b*nlogn
```

### 다양한 알고리즘의 적용 주제들
문교수님이 박사과정할때만 해도, 배고프고 비실용적이라는 인식이 많았는데 이제 많이 변했음. 수학과 통계도 옛날엔 많이 이론위주였는데 많이 Application이 되었듯이 알고리즘도 같음.

- 카 네비게이션: 최단경로 알고리즘
- 스케줄링: TSP, 차량 라우팅, 작업공정, etc
- Human Genome Project: 매칭, 계통도, Functional analysis, etc
- 검색: DB, 웹페이지, etc
- 자원의 배치
- 반도체 설계: Partitioning, placement, routing, etc
- 등

TSP는 NP-hard 문제임. NP-hard, 그리고 NP-hard에 약간의 조건이 추가된 NP-complete에 대해서도 배움. NP-complete 군에 속하는 모든 문제는 바꾸어 풀 수 있음. NP-complete 군에 속하는 문제중 하나의 솔루션으로 다른 문제들도 같이 풀 수 있음. NP-complete 문제중 아직 단 한개도 현실적인 시간 안에 풀 수 있는 알고리즘이 발견되지 않았음. NP-complete 문제를 현실적인 시간 안에 풀지 못할거다라는 추측을 다들 강하게 하고있는데, 증명되진 않고있음.

차량 라우팅 문제는 두개의 NP-hard 문제가 중첩된 문제이다.

10년 전에 반도체 라인을 최적화하는 문제를 풀었었는데, 풀다보니 두개의 NP-hard 문제를 발견했음. 그 전까진 그 NP-hard 문제를 풀지 못하고 숙련공들의 감에 의존해서 풀고있었음.

Human Genome Project에서 알고리즘 엔지니어들이 기여를 많이 했음. String matching이 많이 쓰였다. DNA 시퀀스를 여러개로 잘게 잘라 각각의 작은 조각을 분석한 뒤, 전체를 추정하는 방식으로 분석했음.

계통도. 동물세포의 핵 DNA와 미토콘드리아 DNA를 분석해, 인류와 생물이 어떻게 분화했는지 추적할 수 있게 됨. 이 때 정확한 계통도는 알 수 없고 가장 확률이 높은 계통도를 사용하게되는데, 어려운 최적화 문제이고 NP-hard이다. 90년대에 에이즈 유전자 분석을 해서 한국에 에이즈가 어떻게 유입되었는지 분석한적 있었다. 분석결과 두명의 사람이 해외에서 독자적으로 에이즈에 감염되어왔다는 결론이 났는데, 이것도 알고리즘으로 푼것이다.

아마존에선 물류창고의 물건들을 자동으로 배치하고 관리하는데, 이런것도 최적화 문제이다.

80년대에 알고리즘 전공자들이 제대로된 대우를 받지 못하던 시절, VLSI CAD 분야에서 최적화 전문가들을 많이 고용했었다. 사람이 디자인하기엔 너무 크기때문에 대부분 알고리즘이 디자인한다. 삼성전자 한 회사에서 한해에 지불하는 VLSI CAD 이용료가 1000억이 넘음.

### 알고리즘을 왜 분석하는가
- 무결성을 확인하기위해
- 자원을 얼마나 효율적으로 쓰고있는지 파악하기위해

자원은 시간, 메모리, IO 대역폭 등 다양하다.

크기가 작다면? 알고리즘의 효율성이 그다지 중요하지 않다. 비효율적인 알고리즘도 별 상관 없음. 크기가 충분히 큰 문제여야 알고리즘의 효율성이 중요해진다. 비효율적인 알고리즘이 치명적인 결과를 냄

입력의 크기가 충분히 큰 경우에 대한 분석을 점근적 분석, Asymptotic analysis라고 한다. 이 수업에서 점근적인 분석이라는 말이 앞으로 아주 많이 나온다

### Asymptotic Analysis
입력의 크기가 충분히 큰 경우에 대한 분석. 이미 알고있는 점근적 개념의 예: lim n→∞ f(n)

알고리즘의 복잡도를 표현하기 위한 점근적 notation에선, 계수를 중요하게 사용하지 않는다. 최고차항을 제외한 차수가 낮은 항도 모두 무시한다. 왜냐면 n이 충분히 크면 최고차항만이 큰 차이를 내서.

Big O, Big Omega, Big Theta, little omega, little o 다섯가지 표기법이 있음.

Big O, Little O 두개가 관련이 있고, Big Omega, Little Omega 두개가 관련이 있음.

Big O, Big Omega는 대칭관계, Little O, Little Omega는 대칭관계. Big Theta는 Big O와 Big Omega의 교집합

### Asymptotic Notations
#### Big O
Big O: O(g(n)), 기껏해야 g(n)의 비율로 증가하는 함수.

ex: 3n^2 + 2n, 7n^2 - 100n, n^2 모두 O(n^2). 원래는 "n^2 ∈ O(n^2)" 이렇게 써야하는데, 관행적으로 "n^2 = O(n^2)" 이렇게 표기하기도 함. 같은 뜻임. "O(n^2) = n^2" 이렇게 쓰지는 않은다. 주의

Formal definition: O(g(n)) = {f(n) | ∃c > 0, n₀ ≥ 0 s.t. ∀n ≥ n₀, f(n) ≤ cg(n) }

"∃c > 0, n₀ ≥ 0 s.t. ∀n ≥ n₀"는 "충분히 큰 n에 대해서"를 수학적으로 쓴것임.

직관적 의미: f(n) = O(g(n)) ⇒ f가 g보다 빠르게 증가하지는 않는다 이런 뜻임

Big O notation을 쓰면 가능한한 타이트하게 써야한다. nlogn + 5nㅇ = O(nlogn) 인데 O(n^2)라고 써도 틀리진 않는다. 하지만 엄밀하지 않은 만큼 정보의 손실이 일어난다.

#### Big Omega
Formal definition: Ω(g(n)) = {f(n) | ∃c > 0, n₀ ≥ 0 s.t. ∀n ≥ n₀, f(n) ≥ cg(n) }

직관적 의미: f(n) = Ω(g(n)) ⇒ f는 g보다 느리게 증가하지는 않는다. O(g(n)) 과 대칭적

#### Big Theta
Formal definition: Θ(g(n)) = O(g(n)) ⋂ Ω(g(n))

직관적 의미: f(n) = Θ(g(n)) ⇒ f는 g와 같은 정도로 증가한다.

Big Theta로 이야기할 수 있으면 Big Theta로 이야기해야한다. 제일 Tight한 표기법이기 때문. 그러나 Big Theta로 이야기할 수 없는 경우가 많다.

#### Little O
Formal definition: o(g(n)) = {f(n) | lim n→∞ f(n)/g(n) = 0 }

직관적 의미: f(n) = o(g(n)) ⇒ f는 g보다 느리게 증가한다. (같은 경우가 빠졌음)

o(n^2) = { 9n + 4, nlogn, n^1.99, ... }

#### Little Omega
Formal definition: o(g(n)) = {f(n) | lim n→∞ f(n)/g(n) = inf }

직관적 의미: f(n) = ω(g(n)) ⇒ f는 g보다 빠르게 증가한다. (같은 경우가 빠졌음)

ω(n^2) = { 3n^3, n^2.01, 2^n, ... }

#### 예시
- Selection sort: Θ(n^2)
- Heap sort: O(nlogn)
- Quick sort: O(n^2), 평균 Θ(nlogn), At worst-case: Θ(n^2)

"Comparison sort는 최악의 경우 Ω(nlogn)이다"라는 정리가 있음.

### 시간 복잡도 분석의 종류
- Worst case: 알고리즘이 가장 느려지게 만드는 입력에 대한 분석
- Average case: 모든 경우에 대한 분석, 제일 분석하기 어려움
- Best case: 알고리즘이 가장 빨라지게 만드는 입력에 대한 분석, 그다지 유용하지 않음

보통은 worst case, average cast 분석을 함. Best case가 드물게 유용한데, insertion sort가 worst, average에 Θ(n^) 이지만, best case에는 Θ(n)임.

#### 저장/검색의 복잡도
1.  배열: O(n), 삽입, 삭제, 검색 중 적어도 하나는 Θ(n)
2.  Binary search tree: 최악의 경우 Θ(n), 평균 Θ(logn)
3.  Balanced binary search tree: 최악의 경우 Θ(logn)
4.  B-tree: 최악의 경우 Θ(logn)
5.  Hash table: 평균 Θ(1), 최악의 경우 Θ(n)

#### 크기가 n인 배열에서 원소 찾기
- Sequential search: Worst Θ(n), Average Θ(n), Best Θ(1)
- Binary search: Worst Θ(logn), Average Θ(logn), Best Θ(1)

&nbsp;

Week 2, Mon
========
## 3. Recurrence and Asymptotic Complexity Analysis, 점화식과 알고리즘의 복잡도

### 점화식, recurrence
어떤 함수를 자신보다 더 작은 변수에 대한 함수와의 관계로 표현한 것.

예시:

- f(n) = f(n-1) + 2
- f(n) = n * f(n-1)
- f(n) = f(n-1) + f(n - 2)
- f(n) = f(⌊n/2⌋) + n (보통 f(n/2) + n 라고 줄여 씀)
- ...

merge sort 수행시간을 점화식으로 표현하면 T(n) = 2*T(n/2) + 오버헤드.

### 점화식의 점근적 분석 방법
1.  반복대치, Iteration
    - 함수를 계속 더 작은 형태로 대치해나가기
2.  추정 후 증명, Guess & Verification
    - 결론을 추정하고, 수학적 귀납법으로 증명
3.  마스터 정리, Master Theoram
    - 형식에 맞는 점화식의 복잡도를 바로 알 수 있다

마스터 정리는 굉장히 유용함

### 소요시간 함수 T(n) 에 깔려있는 가정
1.  n은 양의 정수
2.  T는 단조증가
3.  If we need, Without loss of generality (WLOG), n = a^k 라고 가정할 수 있다.

3번이 왜 성립하냐면, a^k, a^k+1 들에 대해 증명이 성공한다면 a^k와 a^k+1 사이에 있는 숫자들에 대해서도 모두 성립하기때문.

### 반복대치, Iteration
#### Selection sort
T(n) = T(n-1) + n, T(1) 일때 반복대치로 T(n)을 구해보자.

```
T(n) = T(n-1) + n
     = T(n-2) + (n-1) + n
     = T(n-3) + (n-2) + (n-1) + n
     ...
     = T(1) + 2 + 3 + ... + n
     = 1 + 2 + ... + n
     = n(n+1)/2
     = Θ(n^2)
```

Selection sort같은 정렬 알고리즘들이 이런식으로 overhead가 Θ(n)인 점화식을 가져서, 죄다 Θ(n^2) 임.

#### Merge sort
1주차에서 한 머지소트 증명도 반복대치임

#### 네개로 나누기
T(n) = n + 3*T(n/4) 도 반복대치로 풀어서, `T(n) = 4n + (T(1) - 4)*n^k, k = log4/log3 = 0.7924...`로, T(n) = Θ(n) 인 것을 알 수 있다.

### 추정 후 증명, Guess & Verification
#### Merge sort
T(n) = 2T(n/2) + n 의 시간복잡도를 추정 후 증명으로 구해보자.

T(n) = O(nlogn) 이라고 가정해보자. 즉, T(n) ≤ cnlogn.

```
n보다 작은 모든 k에 대해 T(k) ≤ cklogk 가 성립한다고 할 때

T(n) = 2T(n/2) + n
     ≤ 2c(n/2)log(n/2) + n     (귀납적 대치, inductive substitution)
     = cnlogn + (1 - clog2)n
     ≤ cnlogn
```

수학적 귀납법에 의해 모든 충분히 큰 c와 n0에 대해 T(n) ≤ cnlogn 이 성립함. c와 n0는 적당히 크게 고르면 된다. 이 경우 c = 2, n0 = 4.

#### Merge sort 변형
T(n) = 2T(n/2 + 17) + n

답이 O(nlogn) 이라고 가정해보자.

```
n보다 작은 모든 k에 대해 T(k) ≤ cklogk 가 성립한다고 할 때,
n/2 + 17 < n 을 만족하는 n에 대해 (34 < n)
n/2 + 17 ≤ 3n/4 를 만족하는 n에 대해 (68 ≤ n)

T(n) = 2T(n/2 + 17) + n
     ≤ 2cT(n/2 + 17)log(n/2 + 17) + n
     ≤ 2cT(n/2 + 17)log(3n/4) + n
     = c*nlogn + (clog3/4 + 1)*n + 34clog(3n/4)
     ≤ cnlogn    for big enough n
```

역시 c와 n0 모두 충분히 크게 잡아주면 됨. c = 5

#### 직관과 배치되는 예
T(n) = 2T(n/2) + 1.

답이 O(n)라고 가정해보자. T(n) ≤ cn

```
T(n) = 2T(n/2) + 1
     ≤ 2c(n/2) + 1
     = cn + 1         ... 더이상 진행 불가
```

이런 경우 T(n) ≤ cn - 2 라고 가정해야함.

```
T(n) = 2T(n/2) + 1
     ≤ 2c(n/2) - 3
     ≤ cn - 2
```

수학적 귀납법으로 증명할때 보통 base case 증명을 해야한다. 그러나 알고리즘 증명할때엔 base case들은 다들 trivial하게 성립해서 생략하는경우가 많다. c를 자기 원하는대로 설정할 수 있기 때문이다.

### 마스터 정리
점화식이 아래와 같은 형태이면, 마스터 정리에 의해 바로 결과를 알 수 있다.

T(n) = aT(n/b) + f(n)

점화식이 위의 꼴일때 이를 계속 iterate하면

T(n) = Σ aⁱf(n/bⁱ) + n^(loga/logb)

앞 항을 Particular solution이라고 함. Overhead의 총합임.

뒤 항을 Homogeneous solution이라고 함. 크기가 1인 문제를 푸는 횟수. Boundary case를 푸는데에 드는 코스트. 아주 중요한 항. 분석의 잣대가 됨

Topmost overhead의 크기와, homogeneous term의 합을 비교해 시간복잡도를 알아낼 수 있음

#### 마스터 정리의 직관적 의미
T(n) = aT(n/b) + f(n) = Σ aⁱf(n/bⁱ) + n^(loga/logb)

위 식에서 Homogeneous term을 h(n)=n^(loga/logb) 과 같이 정의했을 때

1. h(n)이 더 무거우면 h(n)이 수행시간을 결정함
2. f(n)이 더 무거우면 f(n)이 수행시간을 결정함
3. h(n)과 f(n)이 같은 무게이면 logn*h(n) 이 수행시간이 됨

#### 마스터 정리
어떤 양의 상수 ε에 대해...

1. f(n)/h(n) = O(1/n^ε) 이면, T(n) = Θ(h(n))
2. f(n)/h(n) = Ω(n^ε) 이고, 충분히 큰 모든 n에 대해 a*f(n/b) < f(n) 이면, T(n) = Θ(f(n))
3. f(n)/h(n) = Θ(1) 이면, T(n) = Θ(h(n)logn)

"충분히 큰 모든 n에 대해 a*f(n/b) < f(n) 이면" 이 말은 iteration을 할수록 오버헤드가 작아져야한다는 뜻임. 대부분의 알고리즘들은 이 조건을 만족해줌.

사실 Master theorem의 full 버전은 여기에 하나 더해

- f(n)/h(n) = Θ((logn)^k) 이면, T(n) = Θ(h(n)*(logn)^(k+1))

#### 마스터 정리 예시
- T(n) = 2T(n/3) + c

  f(n) = c, h(n) = n^(log2/log3) = n ^ 0.63..

  1번 케이스에 해당, T(n) = Θ(n^(log2/log3)) = Θ(n ^ 0.63..)

- T(n) = 2T(n/4) + n

  f(n) = n, h(n) = n^0.5

  f(n)/h(n) = n^0.5 이고, 2*f(n/4) < f(n)

  2번 케이스에 해당, T(n) = Θ(n)

- T(n) = 2T(n/2) + n

  f(n) = n, h(n) = n

  f(n)/h(n) = 1

  3번 케이스에 해당, T(n) = Θ(nlogn)

슈트라센 알고리즘 시간복잡도 증명도 마스터 정리로 할 수 있음

&nbsp;

Week 3, Mon
========
2020-03-30

Strassen Algorithm 시간복잡도 증명

#### Naive 행렬곱
행렬을 네개의 조각으로 분할하여 각각을 곱한 뒤 더할 수 있다. (PPT 참고) 하나의 큰 행렬곱을 여덟개의 작은 행렬곱으로 분할할 수 있음.

- r = ae + bf
- s = ag + bh
- t = ce + df
- u = cg + dh

이 경우 T(n) = 8T(n/2) + Θ(n^2) 이다. 마스터 정리에 의해 T(n) = Θ(n^(log8/log2)) = Θ(n^3) 임.

#### Strassen's algorithm
슈트라센 알고리즘은 위의 수식을 잘 조작해서, 하나의 큰 행렬곱을 여덟개가 아니라 일곱개로 줄인것이다.

- P1 = a(g-h)
- P2 = (a+b)h
- P3 = (c+d)e
- P4 = d(f - e)
- P5 = (a + d)(e + h)
- P6 = (b - d)(f + h)
- P7 = (a - c)(e + g)
- r = P5 + P4 - P2 + P6
- s = P1 + P2
- t = P3 + P4
- u = P5 + P1 - P3 - P7

T(n) = 7T(n/2) + Θ(n^2) 가 되어, 마스터 정리에 의해 T(n) = Θ(n^(log7/log2)) 이 됨.

슈트라센에서 이걸 발표했을때, 수학계 사람들이 경악했습니다. 제가 500번을 다시 태어나서 태어날때마다 계속 도전한다 해도, 내머리론 안될겁니다. 머리가 조금씩 좋아진다해도 안될거같아. 이런 사람을 앉혀놓고, 이런사람과 경쟁을 해야하는 환경이 주어진다면, 나는 교수 안합니다. 동해안에 커피숍 하나 차려가지고 책읽고 기타나 치면서 한평생 편안하게 사는걸 선택할겁니다. 내가 유학하던 시절에 박사과정에서 배웠는데, 그때 무슨 생각이 들었냐면, 내가 이걸 손으로 찾진 못하겠지만 이걸 찾는 문제공간 탐색알고리즘을 돌리면 어쩌면 가능할지도 모르겠다는 생각이 들었다. 근데 교수되고나니까 이걸 프로그램짜서 직접 하는게 힘들어지더라구요. 서울대 교수라는 자리가 로드가 굉장히 많은 자리입니다. 학교 밖에서도 굉장히 많은걸 요구하고, 시간적으로 힘듭니다. 똑똑한 학생들이 많으니 할수있는게 많기도 하겠지만.

대학원 유전알고리즘 수업에서 플젝으로 TSP나 지수귀문도를 자주 풉니다. TSP는 굉장히 유명한 NP-hard이고, 지수귀문도도 아마 NP-hard 군에 속할건데, 조선시대에 영의정을 지낸 최석정이 고안했다고 알려진 문제입니다. TSP보다 더 어려운 문제임. 이걸 유전알고리즘으로 푸는겁니다. 근데 이것과 아울러서 또 학생들한테 주는게, 슈트라센 알고리즘이라는게 있다, 슈트라센처럼 7개 곱셈으로 행렬곱을 최적화할 수 있는지 유전알고리즘으로 도전해보고싶은사람은 도전해봐라 같이 줍니다. 이거 하는사람은 TSP나 지수귀문도 안해도 된다. 한학기에 한둘씩 이걸 도전하는데, 모든 학생들이 예외 없이 처참한 결과를 냈습니다. 슈트라센 도전했던 사람들이 모두 C를 받았음. 도전정신이 굉장히 강한 학생들이었기때문에 높은 점수를 주고싶은데, 다들 프로젝트를 시작했다고 볼 수 없는 수준에서 그쳐서, 높은 점수를 줄수가 없었음.

그러다가, 2000년대 중반에 학부생 한명이 대학원 수업에 와서 유전알고리즘으로 이걸 도전하겠다고 했다. 본인이 유학을 가겠다고 하는데, 이걸 도전하겠다고 해서 걱정했다. 유학가는데엔 학부 학점이 중요하니. 이거 했다가 C 받으면 어쩌지? 이 학생은 결국 시작했고, 다른 학생들과 다르게 나를 찾아오기 시작합니다. 이거 모든경우의수를 다 하는데엔 영겁의 시간이 걸리고, 선형대수 Rank 테크닉같은걸 동원해서 공간을 좀 줄이면 다 하는데에 데스크탑 PC로 6700만년정도 걸리겠더라구요. 근데 이학생이 5월 중순경에 연락도 없이 뛰어와서 찾았다는겁니다. 믿지 않았어요. 근데 오류가 없는거에요. 그렇게 Strassen's Algorithm과 다른 방법으로 해를 하나 더 찾은겁니다. 그래서 고무가 되어서 더 열심히 찾고, a b c 계수로 {-1, 0, 1} 뿐만 아니라 {-1, -0.5, 0, 0.5, 1} 도 허용하면서 문제공간을 늘려서 더 찾고 그랬는데, 적어도 슈트라센 알고리즘과 다른 608가지 방법이 더 있다는것을 찾아냈습니다. 슈트라센은 608개중에 하나, 위노그라드는 608개중에 다른 하나를 찾은거였습니다. 그래서 그 결과를 학부생이 IEEE Transactions on Evolutionary Computation에 게재했습니다. 지금은 IEEE Transactions on Evolutionary Computation가 IEEE 저널중 Impact Factor가 가장 큰 저널입니다. 그 학생은 미국유학을 가서도 계속 논문을 집필했고, 결국 publish 됩니다.

[Oh, Seunghyun, and Byung-Ro Moon. "Automatic reproduction of a genius algorithm: Strassen's algorithm revisited by genetic search." IEEE Transactions on Evolutionary Computation 14.2 (2009): 246-251.](http://rosaec.snu.ac.kr/publish/2010/T2/OhMo-TEC-accepted.pdf)

컴퓨터의 공간탐색능력은 대단하다. 알파고도 문제공간을 랜덤으로 근사적으로 탐색하는 문제공간 탐색엔진이라고 볼 수 있음. 딥러닝은 탐색하지 않아도 되는 문제공간을 줄이는데에 사용되는것이고. 알파고 나왔을때 많은 바둑 기사들이 일자리를 잃게된다고 탄식했지만 전 신문칼럼에 그렇지 않을거라고 글을 썼죠. 인간과 AI가 공존할거고, 인간의 착상 방법이 알파고로 인해 더 넓어질거다. 그리고 실제로 그렇게 되었습니다. 지금 프로기사들은 인공지능 바둑 패키지 없이는 공부를 할 수 없게 되었습니다.

&nbsp;

## 4. 정렬
정렬 자체도 중요하지만, 정렬문제를 푸는 다양한 관점, 그 과정에서 생각하는 방법을 배우는게 더 중요합니다. 재귀적 사고를 배우는데에 아주 좋습니다.

대부분 O(n^2)과 Ω(nlogn) 사이이다. 그러나 Input이 특수한 성질을 만족하면 Θ(n) 정렬도 가능하다. 예: 입력이 -O(n)과 O(n) 사이일경우 Θ(n)인 counting sort가 가능.

n^2짜리 원시적인 정렬 알고리즘을 먼저 배우고, 그 다음은 nlogn짜리 advanced 정렬 알고리즘, 그다음엔 특수한 성질을 만족하는 경우의 n짜리 정렬 알고리즘에 대해 배운다.

그리고 Comparison sort는 항상 Ω(nlogn)에 속하는데, 이 정렬의 하한성에 대해서도 배운다.

### 원시적인 Sorting 알고리즘들의 재조명
알고리즘에 보는 시각에 크게 두개가 있다

- flow 중심
- 관계 중심

원시적 정렬 알고리즘들은 대부분 flow 중심이다. 이걸 관계 중심으로 다시 생각해보자. 생각하는 방법에 대한 좋은 연습자료다.

#### Selection sort
각 루프마다..

- 최대 원소를 찾는다
- 최대 원소와 맨 오른쪽 원소를 교환
- 맨 오른쪽 원소를 제외한다

이 과정을 하나의 원소만 남을때까지 반복.

T(n) = n(n-1)/2 = Θ(n^2). Worst, Average, Best 무관하게 Θ(n^2) 임

보통 Selection sort를 설명할때 2중 loop로 표현하지만, 사실 재귀적인 성격이 있음. 재귀를 한번 돌때마다 배열의 길이가 1 줄어드는 재귀함수임.

```
fn selection_sort(list: Vec<T>) {
     // Base case
     if list.len() == 1 { return list }

     // Overhead
     list에서 가장 큰 수 list[k] 를 찾는다
     list[k]와 list[-1] 스왑

     // 재귀
     return selection_sort(list[..-1])
}
```

수학적 귀납법으로 증명할 수 있다.

증명: 입력으로 정렬되지 않은 배열 A0가 주어졌다. 배열 A0의 길이는 n이다. selection sort pass를 i번 돈것을 Ai 라고 하자.

Corollary 1: ∀i ∈ {1, 2, ... , n}, A(i-1)[-(i-1)..] = Ai[-(i-1)..] 이다. A(i-1)을 Ai로 바꾸는게 i번째 selection sort pass인데, i번째 selection sort에선 A(i-1)[-(i-1)..] 범위는 조작하지 않기 때문이다.

Corollary 2: ∀i ∈ {1, 2, ... , n}, Ai[-i] 은 Ai[..-i] 안에있는 모든 원소들보다 크거나 같다. i번째 selection sort pass가 하는 일 중 하나가, A(i-1)[..-(i-1)] 에서 제일 큰 원소를 Ai[-i]에 대입시키는 것이기 때문이다.

명제 p(i) = "Ai[-i..]는 단조증가한다" 가 있을때, p(i)가 ∀i ∈ {1, 2, ... , n} 에 대해 성립함을 수학적 귀납법으로 증명해보자.

1.  p(1)는 참이다.

    A1[-1]은 하나짜리 배열이므로 단조증가한다.

2.  ∀i ∈ {1, 2, ... , n-1}, p(i)가 참이면, p(i+1)도 참이다.

    Corollary 2에 의해, Ai[-i]는 Ai[..-i] 에 있는 모든 원소들보다 크거나 같다.

    i번째 Selection sort의 원리 상, A(i+1)[-(i+1)] 은 Ai[..i] 에 있는 원소다.

    ⇒ Ai[-i]는 A(i+1)[-(i+1)]보다 크거나 같다.

    Corollary 1에 의해, Ai[-i] == A(i+1)[-i].

    ⇒ A(i+1)[-i]는 A(i+1)[-(i+1)] 보다 크거나 같다.

    ⇒ A(i+1)[-(i+1)..] 는 단조증가한다

수학적 귀납법에 의해 ∀i ∈ {1, 2, ... , n}, Ai[-i..] 는 단조증가한다.

i에 n을 대입하면, An[-n..] = An 은 단조증가하므로 An은 정렬되어있다. Selection sort pass를 n회 반복하면 배열이 정렬됨을 증명하였다.

#### Bubble Sort
T(n) = n(n-1)/2 = Θ(n^2)

형태는 다르지만 Selection Sort와 거의 비슷함. Pass 한번 돌 때마다 풀어야하는 배열의 크기가 1씩 줄어듬. 얘도 재귀적으로 표현하면

```
fn bubble_sort(list: Vec<T>) {
     // Base case
     if list.len() == 1 { return list }

     // Overhead
     for i in 0..list.len() - 1 {
          if A[i] > A[i+1] {
               A[i]와 A[i+1] 스왑
          }
     }

     // 재귀
     return bubble_sort(list[..-1])
}
```

얘도 선택정렬이랑 비슷하게 증명하면 된다.

#### Insertion Sort
Pass 한번 돌 때마다, 정렬된 배열의 크기가 1씩 늘고, 정렬되지 않은 배열의 크기가 1씩 준다.

- Worst case: 1 + 2 + ... + (n-1) = n(n-1)/2 = Θ(n^2)
- Average case: 0.5 * (1 + 2 + ... + (n-1)) = n(n-1)/4 = Θ(n^2)
- Best case: 1 + 1 + ... + 1 = n = Θ(n)

Insertion Sort가 동작한다는걸 고등학교에서 배운 수학적 귀납법으로 증명할 수 있음

- 배열 A[0..0]만 보면: 원소 하나니까 정렬되어있음
- 배열 A[0..k]가 정렬되어있을 때, insertion sort의 루프 1 pass를 돌리면 A[0..k+1] 이 정렬됨

수학적 귀납법에 의해 insertion sort를 계속 돌리면 배열 전체가 정렬된다.

두 버전의 재귀가 있음

```
fn insertion_sort(list: Vec<T>, sorted: usize) {
     // list를 정렬하고싶음
     // list[0..sorted] 는 정렬되어있음

     // Base case
     if sorted >= list.len() - 1 { return list }

     // Overhead
     list[sorted..] 에서 하나를 뽑아 list[0..sorted] 의 적당한 위치에 삽입함

     // 재귀
     return insertion_sort(list, sorted + 1);
}
```

```
fn insertion_sort(list: Vec<T>) {
     // Base case
     if list.len() == 1 { return list }

     // 재귀
     list[..-1] = insertion_sort(list[..-1])

     // Overhead
     list[-1] 을 list[..-1] 의 적당한 위치에 삽입함
}
```

&nbsp;

Week 3, Wed
========
2020-04-01

#### Merge Sort
`T(n) = 2*T(n/2) + θ(n)`, 마스터 정리에 의해 `θ(N logN)`. 분할정복

#### Quicksort
Merge Sort는 일단 반으로 나눈다음, 머지하는 부분(오버헤드)가 그 뒤에 나온다.

반면 퀵소트는 오버헤드가 먼저 있고, 그 다음 양쪽 반을 처리한다. 피벗을 기준으로 배열을 둘로 파티션 하고나면, 두 파티션이 각각 정렬되기만 하면 정렬이 끝난다. 두 파티션의 크기는 같으리라는 보장이 없고, 운나쁘게 가장 큰 원소나 가장 작은 원소를 고르면 길이 n의 배열이 n-1 하나로 줄어들고 두개로 나뉘지 못한다.

partition에 드는 시간은 Θ(n) 인데, 이러부터 아래를 끌어낼 수 있다. 이거 증명이 오늘 수업에서 제일 중요함

- 평균 수행시간: Θ(nlogn)
- 최악의 경우 수행시간: Θ(n^2)

##### 최악의 경우 수행시간 증명
```
T(n) = T(k-1) + T(n-k) + Θ(n)
```

이 때, 최악의 경우를 가정하면 k = 1

```
T(n) >= T(0) + T(n-1) + Θ(n)
     = T(n-1) + Θ(n)
     = T(n-2) + Θ(n-1) + Θ(n)
     ...
     = Θ(1) + Θ(2) + ... + Θ(n)
     = Θ(1 + 2 + ... + n)
     = Θ(n^2)
```

`T(n) >= Θ(n^2)`이므로 `T(n) = Ω(n^2)`

`T(n) = O(n^2)`는 Guess and verification으로 증명해야한다. 

n보다 작은 모든 k에 대해 T(k) ≤ ck^2 가 성립한다고 가정할 때,

```
T(n) = T(k-1) + T(n-k) + Θ(n)
     <= c{(k-1)^2 + (n-k)^2} + Θ(n)
     <= c{0^2 + (n-1)^2} + Θ(n)
     = cn^2 - 2cn + 2 + Θ(n)
     <= cn^2
```

충분히 큰 c를 고르면 2cn이 "2 + Θ(n)"보다 크므로 성립한다.

O(n^2), Ω(n^2) 이므로 Θ(n^2) 임.

##### 평균 수행시간 증명
Key Observation: 파티션이 최대한 반으로 나눠질수록 알고리즘의 품질이 좋았다. 만약 항상 완벽히 반으로 나눌경우 퀵소트의 시간복잡도는 Θ(nlogn) 이다.

평균적인 경우에도 Θ(nlogn) 라는걸 증명해보자.

그냥 수행시간은 아래와 같은데

```
T(n) = T(k-1) + T(n-k) + Θ(n)
```

평균적인 수행시간은 아래처럼 계산된다

```
T(n) = 1/n * sum(T(k-1) + T(n-k) + Θ(n) for k in 1..=n)
     = 1/n * sum(2*T(i) + Θ(n) for i in 0..n)
     = 2/n * sum(T(i) for i in 0..n) + Θ(n)
```

이 이후는 Guess and verification으로 풀어야함.

n보다 작은 모든 k에 대해 T(k) ≤ cklogk 가 성립한다고 가정할 때:

```
T(n) = 2/n * sum(T(i) for i in 0..n) + Θ(n)
     = 2/n * sum(T(i) for i in 2..n) + Θ(n)
     <= 2/n * sum(cilogi for i in 2..n) + Θ(n)
     = 2c/n * sum(ilogi for i in 2..n) + Θ(n)
     <= 2c/n * integral(xlogxdx, x in 1..n) + Θ(n)
     = 2c/n* (f(n) - f(1)) + Θ(n) where f(x) = x^2logx/2 - x^2/4
     = cnlogn + (-cn/2 + c/2n + Θ(n))
     <= cnlogn
```

부분적분이 쓰인다.

&nbsp;

Week 4, Mon
========
2020-04-06

#### Heap Sort
MinHeap, MaxHeap 중 뭘 써도 됨. 여기선 MinHeap으로 설명. Heap은 완전이진트리여서 배열로 나타낼 수 있음.

1.  주어진 Array를 Heap으로 만든다

    ```
    BuildHeap(A, n) {
         for i = n/2 downto 1 {
              percolateDown(A, i, n)
         }
    }
    ```

    힙의 맨 마지막 Leaf에 가까운 노드부터 점검하며, 힙의 성질을 만족하도록 만들어준다. 맨 밑의 자식부터 검사하며 올라오기때문에, 자식들은 항상 Heap의 조건을 만족하고 Heap의 조건을 만족하지 못하는 후보는 무조건 각 subtree의 루트이다. 각 subtree의 루트를 percolateDown 시키며 내리면 된다.

    percolateDown 할 때, 두 자식중 더 작은 자식과 스왑해야 힙 성질을 만족한다.

2.  힙에서 하나씩 pop하며 정렬한다

    Max를 읽은 뒤, Max 자리에 맨 마지막 원소를 덮어쓰고 그 원소를 Percolate-down 하는 방식으로 힙에서 하나를 pop한다.

BuildHeap은 Θ(n)이다. 각 percolateDown은 Θ(log n) 이다. Θ(log n)을 n/2 번 하니까 Θ(nlogn)이라고 생각할 수 있는데, 각 노드마다 높이가 다르기때문에 가중치합 해서 계산해보면 Θ(n)이 나온다.

```
T(n) = sum(n/(2^(h+1)) * O(h) for h in 0..=log_2 n)
     = O(n * sum(h/2^h for h in 0..=log_2 n))
     = O(n)
```

#### Decision Tree Models of Computation
Comparison Sort의 시간복잡도는 제일 좋아봤자 Ω(nlogn)이다. 왜일까?

증명을 위해 배열의 입력이 모두 다르다고 가정해보자. 이렇게 가정해도 일관성을 해치지 않는다.

1. 입력이 n개일경우, n개의 수를 정렬하는 디시전 트리의 리프 노드는 총 n!개 존재한다.
2. 높이가 h인 바이너리트리는 최대 2^h 개의 리프를 가진다.

1과 2에 의해 이것이 성립한다: n개의 수를 정렬하는 디시전 트리의 높이는 log_2 (n!) 이다.

이때 Stirling 근사에 의해 아래가 성립한다

```
n! ~= (2pi*n)**0.5 * (n/e)**2 * (1 + Θ(1/n))
   > (n/e)**n

log_2 (n!) > log_2 ((n/e)**n)
           = nlog_2n - nlog_2e
```

이를 guess and verification에 활용하면 증명 가능.

Week 4, Wed
========
2020-04-08

Θ(n) 정렬에 대해 알아보자.

#### Counting Sort
원소의 크기가 -O(n) ~ O(n) 범위 내에 있음, Θ(n)

#### Radix Sort
원소들이 k 이하 자리수를 가짐, Θ(kn)

각 자리수별로 따로따로 stable sort를 수행함. Stable sort 여야함!

이 때, 각 자리수별로 소팅하는거를 비교정렬로 할 필요가 없다. 경우의 수가 몇개 없으므로 Counting Sort 하거나 Bucket Sort 하면 됨.

#### Bucket Sort
n개의 원소들이 Uniform하게 분포되어있으면 됨. 정수 아니어도 됨.

1. N개의 버킷에 원소들을 각각 나눠 집어넣음
2. 각 버킷 안의 원소들을 정렬함

평균 수행시간 증명:

```
각 버킷 안에 들어가는 원소 수의 기대값은 n_i = np = 1

분산 σ^2 = npq = np(1-p) = 1 - 1/n (이항분포)

σ^2 = E(n_i^2) - E(n_i)^2

E(n_i^2) = σ^2 + E(n_i)^2
     = (1 - 1/n) + 1
     = 2 - 1/n
     = Θ(1)
```

E(n_i^2)가 1이여서, 버킷 안에 있는 원소들을 무식하게 n^2 정렬해도 각 버킷별 예상 정렬 수행시간은 Θ(1) 이다.

&nbsp;

## 5. 선택 알고리즘
배열에서 i번째 작은 수 찾기

두가지 버전을 배운다

1.  평균적 Θ(n) 알고리즘
2.  최악의 경우에도 Θ(n) 인 알고리즘

상수 k번째 작은 수 찾기는 쉽다. 숫자를 읽으며, 가장 작은 k개의 수를 모두 저장하고있으면 됨.

근데 n/3번째 수 찾기 이런건 저렇게 단순하게는 안됨. 어떻게 할까?

#### Θ(n^2) 방법
1번째 찾아서 제외하고, 2번째 찾아서 제외하고, ...

T(n) = T(n-1) + Θ(n)

#### 평균 Θ(n) 방법
배열을 피벗 기준으로 파티션함. 파티션하고나면 피벗이 배열에서 x번째로 작은 원소라는것을 알 수 있다.

- i == x: 찾았음
- i < x: 피벗이 더 큼, 오른쪽 배열에서 i번째 찾으면 됨
- i > x: 피벗이 더 작음, 왼쪽 배열에서 i-x번째 찾으면 됨

아래는 증명

```
일단 파티션에 드는 시간복잡도때문에 T(n) >= cn 은 자명하다.

추정후 증명으로 T(n) <= cn 을 증명하면 된다.

T(n) = 1/n * sum(max(T(k-1), T(n-k)) for k in 1..=n) + Θ(n)
     = 1/n * sum(T(max(k-1, n-k)) for k in 1..=n) + Θ(n)
     = 2/n * sum(T(k) for k in floor(n/2)..n) + Θ(n)
     <= 2/n * sum(ck for k in floor(n/2)..n) + Θ(n)
     = 2c/n * { (n-1)n/2 - (floor(n/2)-1)*floor(n/2)/2 } + Θ(n)
     <= 2c/n * { (n-1)n/2 - (n/2-2)*(n/2-1)/2 } + Θ(n)
     = cn - cn/4 + c/2 - 2c/n + Θ(n)
     <= cn
```

cn/4가 c/2-2c/n보다 훨씬 커지게 고를 수 있다.

퀵소트랑 닮았고, 퀵소트와 동일하게 파티션을 한쪽 끝으로 고를경우 Θ(n^2)가 된다.

#### 최악에도 Θ(n) 방법
수행시간은 분할의 균형에 영향을 받는다.

- 분할이 항상 1:1 이면 T(n) = T(n/2) + Θ(n) => T(n) = Θ(n)
- 분할이 항상 1:3 이면 T(n) = T(3n/4) + Θ(n) => T(n) = Θ(n)

idea: 최악의 경우에도 분할이 되게 하자. 분할의 균형을 유지하기 위한 오버헤드가 Θ(n) 이하이면 괜찮다.

1. 원소의 총 수가 5개 이하면 naive로 찾는다
2. 전체 원소들을 5개의 원소를 가진 k=ceil(n/5) 개의 그룹으로 나눈다
3. 각 그룹에서 중앙값을 찾는다: m1, m2, ..., mk
4. m1, m2, ..., mk 사이에서의 중앙값 M개를 찾는다. k가 짝수면 중앙값 두개중 아무거나 고름
5. M을 피벗으로 쓰면 된다.

피벗 고르는 과정이 꽤 고통스러워 보이지만, 아무튼 Θ(n)에 찾은거라 괜찮다. 피벗을 이렇게 고르면 3n/10 ~ 7n/10 정도로 균형있게 나눈다는것이 보장된다.

3n/10 보장되는 이유: M은 m1, m2, ..., mk 에서의 중앙값이다. 우연히 가장 작은 M을 고른다고 해도 m1, m2, ..., M 중 세개의 원소들은 M보다 작다. k/2 * 3 이므로 3n/10 이다.

```
T(n) = T(ceil(n/5)) + T(7n/10 + 2) + Θ(n)
     <= T(n/5+1) + T(7n/10 + 2) + Θ(n)
     <= c(n/5+1) + c(7n/10 + 2) + Θ(n)
     = cn - cn/10 + 3c + Θ(n)
     <= cn
```

추정후 증명으로 Θ(n)이 나온다.

&nbsp;

Week 5, Mon
========
2020-04-13

## 6. 검색 트리
### 바이너리 서치 트리
탐색, 삽입은 쉽고 아는거므로 생략. 어려운거는 삭제이다. 아래의 세 케이스로 나눠 처리한다. 삭제하고자 하는 노드가 r일때:

1.  r이 leaf인 경우: 자명
2.  r의 child가 1개인 경우: 자식과 바꿔치기
3.  r의 child가 2개인 경우: 오른쪽 child의 minimum을 r로 바꿔치기

하나의 개념을 반복해서 익히거나, 다른 관점으로 바라보고, 새로운 관점으로 질문을 던지고, 관점을 비틀어서 바라보고, 이렇게 새롭게 바라볼때마다 이해의 차원이 증가합니다. 여러분들이 앞에 똘망똘망하게 쳐다보면서 앉아있지 않으니, 이런 이야기를 하는게 신명나지 않네요.

대답을 얻지 않아도 질문만으로도 도움이 됩니다.

하나의 개념이 있을때 어떤 사람은 이걸 하나의 정의로만 이해하고있을수도 있고, 어떤 사람은 여러 정의, 여러 질문, 여러 관점으로 이해하고있을수도 있습니다. 이 이해의 vector의 차원이 높아지면 어떤 설명을 하지 않아도 개념이 이미지로 이해될 수 있고, 정의부터 떠올릴 필요 없이 real time으로 이야기할 수 있게 되죠. 이 개념을 조합해서 이해해야하는 상위개념을 이해하거나 떠올리려면, 하위 개념을 빠르고 익숙하게 떠올릴 수 있어야 하는데, 정의부터 하나하나 떠올려야 하는 상태라면 그렇게 하기 어렵습니다.

여러분이 없어도 있다고 상상하고 이런이야기를 해봤는데 역시 별로 만족스럽지 않습니다. 오늘 중앙일보에 관련 칼럼을 썼는데 관심있는 사람은 읽어보길 바랍니다.

#### Theorem
빈 BST에 n개의 입력을 넣으면, 평균적으로(입력의 퍼뮤테이션 등장확률이 모두 같을 때) O(nlogn) 소모된다.

Internal Path Length(IPL), BST에서 insertion을 하는 비용들의 합, 이 경우 각 노드의 깊이가 됨.

D(n)을 n개 노드의 평균 IPL이라 할 때, D(0) = 0, D(1) = 1

루트 노드가 전체에서 k등일때, 왼쪽에는 k-1개가 있고 오른쪽에는 n-k개 있을것임. 왼쪽 비용 평균은 D(k-1) 인데, 루트의 자식이므로 k-1이 더해짐. 오른쪽도 D(n-k) 에 n-k 가 더해짐.

Guess and verification으로 증명 가능

```
D(n) = 1/n * sum(D(k-1) + k-1 + D(n-k) + n-k for k in 1..=n) + 1
     = 2/n * sum(D(k) for k in 0..n) + n
     = 2/n * sum(D(k) for k in 2..n) + Θ(n)
     <= 2/n * sum(cklogk for k in 2..n) + Θ(n)
     <= 2/n * integral(cxlogxdx, x in 1..n) + Θ(n)
     = 2c/n * (f(n) - f(1)) + Θ(n) where f(x) = x^2logx/2 - x^2/4
     = cnlogn - cn/2 + c/2n + Θ(n)
     <= cnlogn
```

&nbsp;

Week 5, Wed
========
2020-04-15

없음

&nbsp;

Week 6, Mon
========
2020-04-20

### Red-Black Tree
2-3-4 Tree와 동치. 아래의 조건을 모두 만족하면 RBTree.

1.  Root는 black
2.  모든 Leaf (NIL) 노드는 black
3.  Red node의 자식은 무조건 black (연속된 red 노드 없음)
4.  루트에서 Leaf에 도달하기까지 black의 수는 모두 같음 (black height는 일정함)

Edge에 색칠하는 버전도 있는데 관점만 다른거고 같다.

#### 시간복잡도 증명
n개 노드를 가진 RBTree가 있을때, 최악의 경우에도 깊이가 O(log n)이다.

1. 4번 성질에 의해, black height가 b인 RBTree는 노드 수 n은 최소 2^b - 1 개이다: n >= 2^b - 1
2. 3번 성질에 의해 black height가 b인 RBTRee의 height는 최대 2b이다: b >= h/2

1과 2에 의해:

```
n >= 2^b - 1 >= 2^(h/2) - 1
=> h <= 2 log(n+1)
```

#### 삽입
평범한 BST라고 생각하고 삽입한 후, 삽입한 노드 x를 red로 칠한다. 이때 x의 부모가 p, x의 sibling을 y, p의 부모가 p2, p의 sibling을 s라고 하자.

1.  p is black: 끝
2.  p is red: y는 없거나 black이어야 함
    1.  s is red: p와 s를 black으로 바꾸고, p2를 red로 바꾼다. p2와 p3 가 연속으로 red일 수 있는데, 재귀적으로 해결한다.
    2.  s is black, x가 right child: `y/p\x` 를 rotate해서 `y/p/x` 로 만든다. p와 x가 연속으로 red가 되는데 재귀적으로 해결한다.
    3.  s is black, x가 left child: `x/p/p2`를 rotate해서 `x/p\p2`로 만든다. 끝

삽입할 위치 찾는데에 Θ(logn), 그 뒤 RBTree 고치는데에 O(logn), 총 Θ(logn)

#### 삭제
7개 case가 나와서 복잡하다. 5개로 통폐합 가능하다.

삭제될 노드 m이 자식이 없거나 자식이 하나뿐이라고 가정해도 된다. 자식이 두개인 경우 right subtree의 minimum인 z의 값을 m에 덮어쓴 뒤 z를 삭제하는데, 결국 자식이 없거나 하나인 노드를 삭제하는 문제로 환원되기 때문이다.

m에 자식이 있을경우 그 자식을 x라고 부르자.

1.  m is red: m이 red면 m의 좌우 자식은 모두 검정 NIL이어야함. 걍 m 지우면 됨
2.  m is black:
    1.  m에 자식 x가 있음: x는 무조건 red, m에 x 값을 덮어쓰고 x 삭제
    2.  m 자식 없음: 일단 m을 지우고 아래 문제로 넘어간다.

문제: 부모 p의 두 자식 x, s가 있을 때 x의 black height가 1 작아졌다. 이 상황에서 RBTree 조건 맞추도록 고치기

s 자식의 색을 l, r이라 할 때, x, p, s, l, r의 경우의 수를 모두 나열하면 7개가 나오고, 그중 같은것을 통폐합하면 5개가 된다. 5개 경우에 맞춰 처리해야한다. PPT 참고

### Optimal Binary Search Tree (Static)
각 키가 나타나는 확률이 주어졌을 때, 최적의 BST가 무엇인지를 결정할 수 있음. DP에서 배워보자

&nbsp;

Week 6, Wed
========
2020-04-22

### B-Tree
k-ary tree. Tree Height를 최소화하자. B-Tree 조건

1.  루트를 제외한 모든 노드는 ceil(k/2) ~ k 개의 키를 가짐
2.  모든 리프노드는 같은 깊이를 가짐

탐색하는법은 아니까 생략

#### 삽입
숫자가 들어갈 자리에 일단 넣어본다. Overflow가 안났다면 끝.

Overflow가 날 경우, 인접한 sibling들에게 넘치는 숫자를 redistribute해준다. 넘치는 숫자를 인접한 sibling에게 주며, parent도 고쳐야함.

인접한 sibling이 꽉차서 redistribute를 못하면? 가운데에 있는 수를 골라 split한다음, 가운데에 있는 수를 parent한테 넘겨준다.

이때 parent에서 overflow가 날 경우? 재귀적으로 푼다.

삽입할 위치 찾는데에 Θ(logn), 그 뒤 BTree 고치는데에 O(logn), 총 Θ(logn)

k가 1000~2000만 되더라도, leaf node에 있는 키의 수가 전체 키의 99.9%를 차지하게된다.

#### inner node 삭제
삭제하려는 노드가 x일때, in-order successor z의 값을 x에 덮어쓴 뒤 x를 삭제한다. 이렇게 하면 leaf node 삭제 문제로 환원된다.

#### leaf node 삭제
일단 지워본다. Underflow 안났다면 끝

Underflow 날경우, 인접한 sibling에서 redistribution 시도.

redistribute 못하면? redistribute 실패한 그 sibling과 머지한다. 머지하면서 부모에 있던 키 하나 빼오기.

이때 parent에서 underflow가 날 경우? 재귀적으로 푼다.

- 삭제 원소 검색: Θ(logn)
- 삭제 원소가 leaf node에 있고 수선 없이 해결될 때: Θ(1)
- in-order successor 찾기: O(log n)
- 수선: O(log n)

삭제 시간복잡도: 삭제 원소 검색 제외하면 O(logn), 삭제 원소 검색 포함하면 Θ(logn)

BTree는 주로 디스크 인덱스용으로 쓰이는데, 트리 일부를 메모리에 캐싱하는 방식으로 디스크 액세스 회수를 최적화할 수 있다.

&nbsp;

Week 7, Mon
========
2020-04-27

## 7. 해시 테이블
평균 Θ(1) 색인! 대신 최소원소 찾고 이런건 안됨. 해시 함수는 계산이 간단해야하고, hash table에 고루 저장될 수 있어야한다.

- Division method: h(x) = x mod m

  m은 prime number 테이블 사이즈

- Multiplication method: h(x) = (xA mod 1) * m

  A는 (0, 1) 범위의 실수 상수, m은 소수일 필요가 없어 보통 2^n로 설정

### Collision
- Chaining: 충돌난 자리에 겹쳐서 저장, 추가 자료구조 필요, hashing 무조건 한번만 함, 많이 안씀
- Open Addressing: 테이블 내 다른 공간에 저장, 추가 자료구조 없음, hashing 여러번 할 수 있음

Open Addressing은 아래중 하나

- Linear probing: 바로옆칸에 저장, Primary clustering에 취약함
- Quadratic probing: ai^2 + bi 칸 옆에 저장. Secondary clustering에 취약함. 초기 해시 함수값이 같아서, probing 할때마다 충돌이 계속 나는거임.
- Double hashing: h_i(x) = (h(x) + if(x)) mod m, 애들마다 다른 폭으로 점프함.

### 삭제
조심해야함! Tombstone을 놓거나, 앞으로 모조리 한칸씩 땡기는 작업을 해야함.

### 시간복잡도 계산
Load factor α = n/m. m: 해시테이블 크기, n: 원소 수. Hash table이 차있는 비율. 검색 효율은 α와 밀접한 관련 있음.

가정:

1.  해시함수 h 시간복잡도는 O(1)

2.  해시함수 h의 결과는 uniform함

    h_0(x), h_1(x), ... h_m-1(x) 가 {0, 1, ..., m-1} 의 permutation을 이룰 때, 모든 permutation은 같은 확률로 일어난다.

3.  모든 입력은 같은 확률로 들어옴

#### Chaining에서의 검색시간
Θ(1+α) = Θ(max(1, α))

#### Open addressing에서의 검색시간
unsuccessful search에서의 probe 수 혹은 insertion에서의 probe 수 기대값은 최대 1/(1-α) 이다.

p_i = occupied slot을 정확히 i번 접근한 뒤 unsuccessful search를 마침
q_i = occupied slot을 최소 i번 접근한 뒤 unsuccessful search를 마침

p_i = q_i - q_(i+1)

q_i = n/m * (n-1)/(m-1) * ... * (n-i+1)/(m-i+1) <= (n/m)^i

```
probe 수 기대값 = 1 + sum(i*p_i for i in 1..)
               = 1 + sum(i*(q_i - q_(i+1)) for i in 1..)
               = 1 + sum(q_i for i in 1..)
               <= 1 + sum(α^i for i in 1..)
               = 1/(1-α)
```

successful search에서의 probe 수 기대값은 최대 1/α * ln(1/(1-α)) 이다.

빈 해시테이블에 원소를 한개씩 입력한다고 해보자. i번째 키를 입력한 직후의 로드팩터는 i/m, i+1 번째 키를 입력할때 probe 수 기대값은 1/(1-(i/m)) = m/(m-i)

모든 키에 대해 평균내면

```
1/n * sum(m/(m-i) for i in 0..n)
= m/n * sum(1/(m-i) for i in 0..n)
<= 1/α integral(dx/m-x, x in 0..n)
= 1/α * ln(1/(1-α))
```

Load Factor가 올라가면 hash table의 효율이 급락함. 보통 load factor에 스레숄드를 50% 이런식으로 정해두고, load factor가 커지면 hash table의 크기를 두배로 늘린다음 모조리 rehashing 함.

&nbsp;

Week 7, Wed
========
2020-04-29

### Minhash
Hash Table을 영리하게 이용했음. 두 집합의 유사성을 빠르게 판별하게 함.

Jaccard Similarity: J(A, B) = |A∩B|/|A∪B|

많은 집합 A1, A2, ..., An 이 있고 이들간의 pairwise similarity를 모두 계산해야한다면 시간이 많이 든다.

Binary vector, Signature vector를 만든다.

- h(x): 해시 함수
- h_min(S) = S 원소 x중 h(x)가 가장 작은 x

이때 아래가 성립한다.

J(A, B) = h_min(A)와 h_min(B)가 같을 확률

해시함수를 하나만 준비하지 말고 충분히 많은 함수를 준비하면, 몬테카를로 메서드로 A와 B의 유사도 측정이 가능함.

동영상에 멍뭉이 짖는 소리가 들림

&nbsp;

## 8. Disjoint Sets
Disjoint Set만 대상으로 한다. 교집합 없음.

링크트리스트, 트리로 만들 수 있음. 강의자료에는 없으나 해시로도 만들 수 있음.

### 링크트리스트 기반
![](https://raw.githubusercontent.com/simnalamburt/i/main/snucse/linked-list-disjoint-set.png)

- 집합의 정의: 하나의 링크트 리스트로 연결되면 하나의 집합으로 간주
- FindSet, 집합 대표 찾기: O(1), 링크트 리스트의 맨 앞 원소를 대표로 사용함, 모든 링크트 리스트 원소가 대표 원소에 대한 포인터를 갖고있음
- Union: 두 리스트 하나로 연결한 뒤 대표 원소 포인터를 모두 업데이트, O(n)
  - Weight을 고려한 Union: 무조건 작은 집합을 큰 집합 뒤에 붙이도록 하면, 대표 원소 포인터 업데이트를 줄일 수 있음

Weighted Union 을 썼을때 아래가 성립함.

m번의 MakeSet, Union, FindSet을 수행했고, MakeSet이 그중 n번이라면, 총 수행시간은 O(m + nlogn)

위 문장을 다르게 해석하면 집합의 원소가 n개라는거임. 

임의의 원소 x가 있을때, x의 대표원소를 가리키는 포인터가 몇번 업데이트될지 생각해보자. Union에서 작은 쪽에 속해야만 포인터가 업데이트된다. x의 대표원소 포인터가 한번 업데이트되었다면, x가 속한 집합의 크기는 최소 두배이상 커진다.

전체 원소 수는 n개이므로, 각 원소는 최대 log_2 n 번 이상 업데이트될 수 없는것이기 때문에, 모든 원소가 제일 많이 업데이트되어봤자 n log_2 n 번 업데이트되는것이다.

알고리즘의 흐름이 아니라 개개의 원소에 몇번의 오퍼레이션이 가해지는지로 분석한다. 새로운 관점!

### 트리 기반
![](https://raw.githubusercontent.com/simnalamburt/i/main/snucse/tree-disjoint-set.png)

더 복잡하지만, 더 효율적임. 근데 우리가 보통 보던거랑 포인터 방향이 반대임. 자식들이 부모를 가리킴.

- 집합의 정의: 하나의 트리로 연결되어있으면 하나의 집합으로 간주
- FindSet, 집합 대표 찾기: O(rank), 루트 노드를 대표로 사용함
- Union: O(rank), 한쪽 트리를 반대쪽 트리 루트노드의 자식으로 넣어준다.

이것만 보면 대충 log N 시간만에 끝나는것 아닌가 싶지만 아래 두개 최적화를 넣으면 매우 강력해진다.

- Rank를 이용한 Union: 모든 노드는 자신을 루트로 갖는 subtree 높이의 상한을 rank라는 이름으로 저장함, union 할 떄 rank가 낮은 집합을 rank가 높은 집합의 자식으로 넣는다. 같은 rank를 가진 집합끼리 union하면 rank가 증가함.
- Path compression: Find-Set을 하면 루트 노드로 타고 올라가야하는데, 기왕 타고 올라가는 김에 만나는 모든 노드에 대해, 루트의 직접적인 자식이 되도록 parent 포인터 업데이트하기

왜 Height가 아니라 상한이라는 표현을 쓰는가? Path compression할 때 rank 다시 업데이트하는건 비싸서. 자기 밑에 뭐가 있는지 알 수 있는 방법이 없다.

Rank 이용한 Union과 Path compression을 사용한 Find-Set을 쓸 때, 아래가 성립한다.

수행시간: m번의 MakeSet, Union, FindSet중 n번이 MakeSet일때 이들의 수행시간은 O(m f(n)) 이다. f(n) = log를 k회 중첩해 log log ... log n <= 1 가 되게하는 최소의 k

f(n)은 몹시 작아서 사실상 상수임.

n = 2^(2^(2^(2^2))) 이면 f(n)은 5

따라서 각 MakeSet, Union, FindSet 연산이 **사실상 상수시간**만에 끝나는것과 같은 효과가 발생한다.

이번 강의에서 심심해서 배경음악을 깔아봤는데 눈치를 채셨는지 모르겠습니다. 저는 귀가 예민해서 강의하다가 자꾸 초점을 놓쳤었습니다.

&nbsp;

Week 8, Mon
========
2020-05-04

## 9. Dynamic Programming
재귀적 해법! 잘쓰면 보약, 잘못쓰면 맹독. 관계중심으로 파악함으로써 문제를 간명하게 볼 수 있다. 그러나 잘못 사용하면 심한 중복호출이 일어난다.

아래 두개가 있으면 DP 쓸 수 있음

- Optimal Substructure
- Overlapping recursive calls

메모아이제이션 쓸 수 있다!

### 행렬 경로 문제
(문제는 PPT 참고)

f(i-1, j)와 f(i, j-1)로 f(i, j)를 낼 수 있다. Θ(n^2)로 답 나옴.

### 돌놓기
i-1열의 패턴 A B C 로 i열의 패턴 A B C 답을 낼 수 있음. Θ(n)

&nbsp;

Week 8, Wed
========
2020-05-06

### 행렬 곱셈 순서
(AB)C와 A(BC)는 결과가 같지만 연산량이 다르다. (Associative) A1, A2, A3, ..., An 을 곱하는 최적 순서는?

- A1 * (A2, ... An)
- (A1A2) * (A3, ..., An)
- (A1A2A3) * (A4, ..., An)
- ...
- (A1 ... Ak) * (Ak+1 ... An)
- ...
- (A1 ... An-2) * (An-1An)
- (A1 ...    An-1) * An

Ak의 차원이 p(k-1) * pk 일때

```
f(i,j) = if i == j: 0
       | if i < j: f(i, k) + f(k+1, j) + p(i-1)*pk*pj
```

문제공간 n^2개, 하나 루프돌때마다 Θ(n) 소요, 총 Θ(n^3) 소모

### 최장 공통 부분순서

```
f(i,j) = if i == 0 || j == 0  : 0
         else if x_i == y_i   : f(i-1, j-1) + 1
         else                 : max(f(i-1, j), f(i, j-1))
```

&nbsp;

Week 9, Mon
========
2020-05-11

### Optimal Binary Search Tree
TODO

PPT 참고

&nbsp;

Week 9, Wed
========
2020-05-13

### Graph Traversal
DFS, BFS

- Tree edge: BFS/DFS에서 vertex를 tree에 편입시킬 때 사용한 edge
- Back edge: 자신의 ancestor로 연결되는 edge
- Forward edge: 자신의 descendant로 연결되는 edge
- Cross edge: ancestor/descendant가 아닌 vertex를 연결하는 edge

무향그래프에서, BFS tree는 forward edge 존재 불가능. DFS에선 cross edge 존재 불가능

유향그래프에서, BFS tree는 forward edge 존재 불가능, 그 외엔 다 가능

### Minimum Spanning Tree: Prim Algorithm
그리디. 점 하나에서 시작해서, 현재 스패닝 트리와 스패닝 트리에 포함되지 않은 간선중 최소 간선을 찾아 S에 포함시키며 relaxation 한다.

힙 쓰면 O(ElogV)

&nbsp;

Week 10, Mon
========
2020-05-18

### Minimum Spanning Tree: Kruskal Algorithm
1.  전체 vertex를 각각 singleton set으로 초기화한다.
2.  간선 집합을 가중치 순으로 정렬한다.
3.  간선집합에서 최소비용 간선을 제거해가면서, 간선의 양쪽 정점이 서로 다른 집합에 속할경우 두 집합을 하나로 합친다.

### 안정성 정리
두 집합을 연결하는 cross edge중 가장 짧은 간선 x가 있을때, x를 포함하는 미니멈 스패닝 트리가 적어도 하나 존재한다.

프림알고리즘과 크루스칼 알고리즘의 근거

귀류법으로 증명함. 쉽알 292페이지에 있음

### Topological Sorting
DAG 정렬하기. Θ(V+E)

```
for i in 0..n:
  incoming edge가 없는 정점 u를 고른다
  A[i] = u
  u와 u의 outdgoing edge를 모두 지운다
```

```
for i = n downto 1:
  outgoing edge가 없는 정점 u를 고른다
  A[i] = u
  u와 u의 incoming edge를 모두 지운다
```

```
모든 정점 v에 대해:
  v를 방문한적이 없으면:
    DFS-TS(v)

def DFS-TS(v):
  v를 방문했다고 체크
  v와 인접한 모든 정점 x에 대해:
    x를 방문한적이 없으면:
      DFS-TS(x)
  연결리스트 R 앞에 v 삽입
```

&nbsp;

Week 10, Wed
========
2020-05-20

### Shortest Paths
단일 시작점 최단경로: 다익스트라, 벨만포드

모든 쌍 최단경로: 플로이드 워셜

다익스트라: 모든 간선이 양수

벨만포드: 음수 간선 허용. 벨만포드는 DP임

### DAG의 Shortest Path
Θ(V+E) = Θ(max(V, E)) 에 구해짐

위상정렬 한번 하고, 위상정렬된 순서로 정점을 보며 outgoing edge들에 대해 relaxation 가능한지 보면 된다.

&nbsp;

Week 11, Mon
========
2020-05-25

### Strongly Connected Components
Digraph의 모든 정점쌍에 대해, 양방향으로 경로가 존재하면 강하게 연결되었다고 한다. 임의의 그래프에서 강연결요소를 찾는것도 문제다.

1. 아무데로부터 DFS를 수행하여, 각 정점 v의 완료시간 f_v를 구한다. 모든 정점을 커버하지 못하면 커버 안된 점으로 반복한다.
2. G^R을 만든다. 모든 화살표가 반대인 그래프.
3. G^R을 대상으로 DFS를 반복수행하되, 아직 방문되지 않은 정점중 f_v가 가장 큰 정점을 시작점으로 한다.
4. 3에서 만든 트리가 강연결요소가 된다.

두 vertex가 한 SCC에 속해있다

iff. 두 vertex가 step 3의 같은 DFS 트리에 속한다.

증명은 쉽알 324p

&nbsp;

## 13. NP-완비
동영상 강의는 시간에 구애받지 않고, 학생들이 괴롭든 말든 마구 구겨넣을 수 있다는걸 알았어요 (?)

- 풀 수 없는 문제: Unsolavable, Undecidable
- 풀 수 있는 문제: Solvable, Deciable
  - 현실적인 시간 내에 풀 수 있는 문제: Tractable
  - 현실적인 시간 내에 풀 수 없는 문제: Intractable

(스펠링 헷갈림) 나이들면 이런게 좀 헷갈립니다.

NP-Complete 문제 군이 intractable에 들어간다고 아직 증명이 안되어있습니다만, 강력하게 추정되고있습니다.

현실적인 시간내에 풀 수 있냐 없냐를 가르는 기준은 Polynomial time입니다.

Yes/No 문제: 그래프 G에서 길이가 k 이하인 해밀토니안 경로가 존재하는가?

최적화 문제: 그래프 G에서 길이가 가장 짧은 해밀토니안 경로는 얼마인가?

NP-Complete/NP-Hard, 현실적인 시간 내에 푸는 방법 아직 없음, 증명 안됨

바꿔풀기. 어떻게 여러 문제들이 같은 논리적 문제군을 이루는가?

1. x = x1x2...xn 은 3의 배수인가?
2. x1+x2+...+xn 은 3의 배수인가?

2가 쉬우면 1도 쉽다.

- 문제 B는 쉽다
- 문제 A는 Yes/No 대답이 일치하는 문제 B로 쉽게 변형된다.
- 위 두개가 만족하면 문제 A도 쉽다.

Poly-Time Reduction

- A의 사례 a를 B의 사례 b로 바꾸는것이...
- 다항시간 내에 변환 가능하고
- 답이 일치하면
- 이를 polynomial-time reduction이라 하고 a <=_p b 라고 한다.

P: Polynomial. 다항시간 안에 Y/N 대답가능

NP: Nondeterministic Polynomial. Yes 대답이 나오는 해가 주어졌을때, 이것의 대답이 Yes가 맞다는걸 다항시간 안에 확인 가능.

NP 보이는건 쉬움

NP-Hard: 모든 NP문제가 L로 다항시간 내에 변환 가능하다.

NP-Complete: L은 NP 이다 and L은 NP-Hard 이다 => L은 NP-Complete다.

정리: 문제 L, NP-Hard 문제 A 에서 A <=_p L 일 경우, L도 NP-Hard이다. Transitivity에 의해 성립함.

TSP 문제가 NP-Hard임을 보이는법: 해밀토니안 사이클 문제 A는 NP-Hard이고, A <=_p TSP 를 보이면 된다.

해밀토니안 사이클 문제 A에서

A에서 연결된 간선은 웨이트 1로, 연결 안된 간선은 웨이트 무한인 완전그래프로 바꾸면 이게 TSP 문제의 인스턴스가 된다.

해밀토니안 사이클 문제 A를 TSP문제의 인스턴스로 바꾸는데에 성공했다. 그러므로 TSP는 NP-Hard다.

NP-Complete 문제들끼리 유향 그래프가 형성되어있음. 부모가 풀리면 자식들도 풀림

&nbsp;

Week 11, Wed
========
2020-05-27

Longest path는 NP-Hard다

- A: s에서 t로 가는 가장 긴 simple path의 길이는 얼마인가
- B: s에서 t로 가는 길이 k 이상인 simple path가 존재하는가
- C: s에서 t로 가는 해밀토니안 경로가 존재하는가

C가 NP-Hard라는게 알려져있음. C에서 B로 변환 가능, B에서 A로 변환 가능

### CLIQUE
- A: 그래프에 크기 K인 complete subgraph가 존재하는가?
- 3SAT: 알려진 NP-Hard

3SAT <=_p A 임

### P NP
P ⊊ NP, P = NP 여부가 안알려져있음.

### 최적화 문제로 확장하기
NP-Complete는 무조건 decision problem인데, NP-Hard는 안그럼

다항식 시간 변환의 재정의: 변환을 다항시간 내에 할수있으며 "답이 같다"

### 근사
NP-Hard는 걍 휴리스틱으로 풀어야한다.

ratio bound: C/C* <= row(n)

- C: 휴리스틱 솔루션 비용
- C*: 옵티멀 솔루션 비용

&nbsp;

Week 12, Mon
========
2020-06-01

## 11. 그리디
그리디가 맞는경우가 간혹 있음. 그 간혹 있는 경우를 배워보자.

### 그리디 보장 안되는 경우
로컬 옵티마에 빠지는 경우

- 이진트리 최대합 경로 찾기: 정보 부족
- 여러 액면가의 동전들이 주어졌을때, 최소 동전 수로 원하는 액수 만들기. 동전의 액면이 그 다음 동전 액면의 배수이면 그리디가 되는데, 그런 보장이 없으면 그리디로 못품

### 그리디가 맞는경우
프림, 크루스칼: 안정성 정리 덕분

회의실 하나가 있을때, 여러명이 회의실 사용신청을 할때 가장 많은 회의가 열리게 하는법: 종료시간이 가장 이름 회의 순으로 배정하면 최적해.

### 매트로이드, Matroid
그리디로 최적해가 보장되는 수학적 구조.

유한집합 S의 부분집합 I에 대해, 아래가 만족되면 매트로이드다.

1. 상속성: A ∈ I, B ⊆ A => B ∈ I
2. 증강성, 교환성: A, B ∈ I, |A| < |B| => A ∪ {x} ∈ I 인 x ∈ B - A 가 존재한다.

#### 매트로이드 예시
S = {a, b, c, d}, I = {{}, {a}, {b}, {c}, {d}}

1 만족함, 2 만족함

#### 매트로이드 예시 2
S = {a, b}, I = {{}, {a}, {b}, {a, b}}

1 만족함, 2 만족함

#### 그래픽 매트로이드
트리의 모음인 숲집합은 매트로이드다. 간선의 부분집합중 사이클이 없는것이 트리임.

#### 매트로이드의 확장
A ∈ I 에 대해, A에 없는 원소 x ∈ S 에 대해 A ∪ {x} ∈ I 이면 x가 A를 확장한다고 함.

A가 더 확장되지 않으면 포화집합, maximal set이라 한다.

매트로이드 I ⊆ 2^S 에 대해, 모든 포화집합은 같은 크기를 가진다. 만약 크기가 다르면 증강성에 의해 모순 발생.

#### 가중치 매트로이드
S의 원소들이 양의 가중치를 갖고있을 때, 원소들의 합을 최대화하는 부분집합 A ∈ I를 찾고자 한다. 아래 알고리즘으로 최적해가 보장된다.

```
A = {}
S의 원소들을 가중치 크기 내림차순으로 정렬
for x in S:
  x가 A를 확장할수있으면?:
    A = A ∪ {x}
```

이거 증명은 교과서 참고.

#### 가중치 공간의 모양은?
로컬 맥시마(봉우리)가 하나이거나, 봉우리에 같은 높이를 가진 값이 여러개 있어야함 (고원, 플라토)

#### 재미있는 성질
가중치 매트로이드에서 서로 다른 최적해가 두개 이상일경우, 답은 다를지언정 그들의 원소 가중치 집합은 반드시 동일하다.

가중치 매트로이드의 문제 공간에서는 단 하나의 봉우리만 존재하고, 거기에 1개 또는 그 이상의 최적해가 존재한다.

&nbsp;

Week 12, Wed
========
2020-06-03

## 12. 문자열 매칭
문자열 A[1..n], 패턴 P[1..m], n>>m 일 떄

### Naive: Θ(nm)
헛일을 많이한다.

### 오토마타: Θ(n)
효율적이다. 오토마타는 Truth table을 2차원 행렬로 만들어서 표현 가능.

수행시간: Θ(n)
총 수행시간: Θ(n + |Σ| m)

오토마타 생성하는데에 Θ(|Σ|m)이 든다. Σ는 문자열 집합

중간고사 없이 기말고사로 가기때문에 오토마타 이부분은 여러분이 그냥 모티베이션만 받아들이고, KMP에 집중을 하길 바랍니다.

### 라빈카프 알고리즘: Θ(n)
문자열 패턴을 수치로 바꾼다.

- Σ = {a, b, c, d, e}
- |Σ| = 5
- a = 0, b = 1, c = 2, d = 3, e = 4 5진수로 만든다.
- "cad" = `2*25 + 0*5 + 3` = 28

a_i를 계산하면 여기에 숫자 빼고 더하고 해서 a_(i+1) 을 계산할 수 있다.

문제: 패턴이 너무 크면 오버플로우 발생.

a_i 를 그대로 쓰는게 아니라 충분히 큰 소수 q로 나눈 나머지 값으로 매칭하기도 한다. 근데 이러면 또 우연히 매칭이 아닌데 값이 겹쳐버릴 수 있음. 그 확률은 n/q

q를 정말 큰 수로 해야겠죠?

### KMP: Θ(n)
오토마타를 이용한 매칭과 동기가 같음.

- 매칭에 실패했을 때 돌아갈 상태를 준비해둔다
- 오토마타를 이용한 매칭보다 준비작업이 단순하다

#### 매칭에 실패했을때 돌아갈곳 준비작업
패턴이 "abcdabcwz"라고 해보자. ㅍabcdabc 다음에서 매칭에 실패하면 abc로 돌아가야한다.

```
패턴으로 프리프로세싱 먼저 함
i := 1 # 본문 문자열 포인터
j := 1 # 패턴 문자열 포인터
while i <= n:
  if j == 0 || A[i] == P[j]:
    i += 1, j += 1
  else:
    j := 𝜋[j]
  if j == m+1:
    A[i-m] 에서 매치되었음을 알림
    j := 𝜋[j]
```

while루프 안에서 하는거 모두 상수시간임. while루프 n번 도니까 Θ(n)

아래는 프리프로세싱 과정

```
j = 1
k = 0 # prefix 포인터
𝜋[1] = 0
while j <= m:
  if k == 0 || P[j] == P[k]:
    j += 1, k += 1, 𝜋[j] := k
  else:
    k := 𝜋[k]
```

### 보이어 무어
앞에애들은 최선의 경우에도 Omega(n)이다. 입력을 다 읽어야하니. 근데 얘는 다 안봐도된다.

패턴의 오른쪽부터 비교한다.

Observation: 아예 패턴에 없는 문자가 있으면, 패턴이 그부분을 통째로 건너뛸 수 있다.

Observation: 현재 입력이 i인데 패턴이 "tiger"다. 그러면 i부터 맞추도록 3칸 점프 가능

Observation: 현재 입력이 a인데 패턴이 "rational"이다. a가 오른쪽에서 6번째와 1번째에 있는데, 1칸만 점프해야한다.

보이어-무어-호스풀 알고리즘: 약간 suboptimal 하더라도 간단하게 처리하는 알고리즘

Worst-case Θ(nm), Best case Θ(n/m), 보통 Θ(n)보다 가볍다.
